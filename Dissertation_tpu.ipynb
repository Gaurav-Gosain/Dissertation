{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Dissertation_tpu.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f353f99404084e4a9f7a87d79bb38684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7041ad2144064f13bc5bb31f35b7f2fd",
              "IPY_MODEL_6ac3c60b187248348cfb63c095216cb8",
              "IPY_MODEL_1fff411e47a14fe988c78c1322529c34"
            ],
            "layout": "IPY_MODEL_e374a921a4dc4933b39a3694879a9f66"
          }
        },
        "7041ad2144064f13bc5bb31f35b7f2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36f1f2a361b14fe488d2c0ca63be6e75",
            "placeholder": "​",
            "style": "IPY_MODEL_075537718d9f4467bf4cd12fc9f58362",
            "value": "Epoch 0:   4%"
          }
        },
        "6ac3c60b187248348cfb63c095216cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faca4d3a928346d1b6e775b7caf7e0ce",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df0f266606d547f799afd4869faff584",
            "value": 1
          }
        },
        "1fff411e47a14fe988c78c1322529c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_940e04a719c94f798503bae8994b50ce",
            "placeholder": "​",
            "style": "IPY_MODEL_575b0b08dca14a369237dbdc6eed19d8",
            "value": " 1200/33271 [27:37&lt;12:18:28,  1.38s/it, loss=60.2, v_num=0]"
          }
        },
        "e374a921a4dc4933b39a3694879a9f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "36f1f2a361b14fe488d2c0ca63be6e75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075537718d9f4467bf4cd12fc9f58362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faca4d3a928346d1b6e775b7caf7e0ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0f266606d547f799afd4869faff584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "940e04a719c94f798503bae8994b50ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "575b0b08dca14a369237dbdc6eed19d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip --quiet install kornia einops plyfile open3d numba test-tube\n"
      ],
      "metadata": {
        "id": "wR5dF2NGhbcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f86afd-62fd-43be-e625-df20647f4754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "# ! pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.10-cp37-cp37m-linux_x86_64.whl\n",
        "! pip install --quiet \"torchvision\" \"torchtext\" \"torch>=1.6, <1.9\" \"torchmetrics>=0.3\" \"ipython[notebook]\" \"pytorch-lightning>=1.3\"\n",
        "# !pip uninstall -y torch\n",
        "# !pip install torch==1.4.0 \n",
        "# !pip install \"pytorch-lightning>=1.3\" \"torch\" \"torchvision\" \"torchtext\" \"torchmetrics>=0.3\"\n",
        "# !pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETSWgw10NU1Y",
        "outputId": "ffc3ca0e-2761-4a1e-9fae-f90929ef264f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.6 kB/s \n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 167 kB/s \n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 91 kB/s \n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 68.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 72.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.4 MB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 77.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 31.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 27.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 25.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 25.9 MB/s \n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Gaurav-Gosain/dissertation_data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8C8hI0-PYr-",
        "outputId": "82fcec2d-f010-4548-9fb4-0faaed38ef20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'dissertation_data' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5UzRaeZoWhde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch-lightning\n",
        "from  pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import sys\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import math\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import itertools as it\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# optimizer\n",
        "from torch.optim import SGD, Adam\n",
        "# scheduler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR, LambdaLR\n",
        "from einops import reduce, rearrange, repeat\n",
        "import torch.nn.functional as F\n",
        "from kornia.utils import create_meshgrid"
      ],
      "metadata": {
        "id": "sw-XgCgY4KWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Functions"
      ],
      "metadata": {
        "id": "2qXUtRMwVFu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SL1Loss(nn.Module):\n",
        "    def __init__(self, ohem=False, topk=0.6):\n",
        "        super(SL1Loss, self).__init__()\n",
        "        self.ohem = ohem\n",
        "        self.topk = topk\n",
        "        self.loss = nn.SmoothL1Loss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets, mask):\n",
        "        loss = self.loss(inputs[mask], targets[mask])\n",
        "\n",
        "        if self.ohem:\n",
        "            num_hard_samples = int(self.topk * loss.numel())\n",
        "            loss, _ = torch.topk(loss.flatten(), \n",
        "                                 num_hard_samples)\n",
        "\n",
        "        return torch.mean(loss)\n",
        "\n",
        "loss_dict = {'sl1': SL1Loss}"
      ],
      "metadata": {
        "id": "FjQ2GKcRmayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Helper Functions"
      ],
      "metadata": {
        "id": "ri1Pt377VKI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abs_error(depth_pred, depth_gt, mask):\n",
        "    depth_pred, depth_gt = depth_pred[mask], depth_gt[mask]\n",
        "    return (depth_pred - depth_gt).abs()\n",
        "\n",
        "def acc_threshold(depth_pred, depth_gt, mask, threshold):\n",
        "    \"\"\"\n",
        "    computes the percentage of pixels whose depth error is less than @threshold\n",
        "    \"\"\"\n",
        "    errors = abs_error(depth_pred, depth_gt, mask)\n",
        "    acc_mask = errors < threshold\n",
        "    return acc_mask.float()"
      ],
      "metadata": {
        "id": "9uxzSvZ-VMT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers"
      ],
      "metadata": {
        "id": "SvrzbMLNV6Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class PlainRAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "                    \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "        super(PlainRAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(PlainRAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                beta2_t = beta2 ** state['step']\n",
        "                N_sma_max = 2 / (1 - beta2) - 1\n",
        "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif self.degenerated_to_sgd:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, warmup = warmup)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                \n",
        "                if group['warmup'] > state['step']:\n",
        "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
        "                else:\n",
        "                    scheduled_lr = group['lr']\n",
        "\n",
        "                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n",
        "                \n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
        "\n",
        "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "#Ranger deep learning optimizer - RAdam + Lookahead combined.\n",
        "#https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
        "\n",
        "#Ranger has now been used to capture 12 records on the FastAI leaderboard.\n",
        "\n",
        "#This version = 9.3.19  \n",
        "\n",
        "#Credits:\n",
        "#RAdam -->  https://github.com/LiyuanLucasLiu/RAdam\n",
        "#Lookahead --> rewritten by lessw2020, but big thanks to Github @LonePatient and @RWightman for ideas from their code.\n",
        "#Lookahead paper --> MZhang,G Hinton  https://arxiv.org/abs/1907.08610\n",
        "\n",
        "#summary of changes: \n",
        "#full code integration with all updates at param level instead of group, moves slow weights into state dict (from generic weights), \n",
        "#supports group learning rates (thanks @SHolderbach), fixes sporadic load from saved model issues.\n",
        "#changes 8/31/19 - fix references to *self*.N_sma_threshold; \n",
        "                #changed eps to 1e-5 as better default than 1e-8.\n",
        "\n",
        "\n",
        "class Ranger(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95, 0.999), eps=1e-5, weight_decay=0):\n",
        "        #parameter checks\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        if not lr > 0:\n",
        "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
        "        if not eps > 0:\n",
        "            raise ValueError(f'Invalid eps: {eps}')\n",
        "\n",
        "        #parameter comments:\n",
        "        # beta1 (momentum) of .95 seems to work better than .90...\n",
        "        #N_sma_threshold of 5 seems better in testing than 4.\n",
        "        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
        "\n",
        "        #prep defaults and init torch.optim base\n",
        "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params,defaults)\n",
        "\n",
        "        #adjustable threshold\n",
        "        self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "        #now we can get to work...\n",
        "        #removed as we now use step from RAdam...no need for duplicate step counting\n",
        "        #for group in self.param_groups:\n",
        "        #    group[\"step_counter\"] = 0\n",
        "            #print(\"group step counter init\")\n",
        "\n",
        "        #look ahead params\n",
        "        self.alpha = alpha\n",
        "        self.k = k \n",
        "\n",
        "        #radam buffer for state\n",
        "        self.radam_buffer = [[None,None,None] for ind in range(10)]\n",
        "\n",
        "        #self.first_run_check=0\n",
        "\n",
        "        #lookahead weights\n",
        "        #9/2/19 - lookahead param tensors have been moved to state storage.  \n",
        "        #This should resolve issues with load/save where weights were left in GPU memory from first load, slowing down future runs.\n",
        "\n",
        "        #self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
        "        #                     for group in self.param_groups]\n",
        "\n",
        "        #don't use grad for lookahead weights\n",
        "        #for w in it.chain(*self.slow_weights):\n",
        "        #    w.requires_grad = False\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        print(\"set state called\")\n",
        "        super(Ranger, self).__setstate__(state)\n",
        "\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n",
        "        #Uncomment if you need to use the actual closure...\n",
        "\n",
        "        #if closure is not None:\n",
        "            #loss = closure()\n",
        "\n",
        "        #Evaluate averages and grad, update param tensors\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Ranger optimizer does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]  #get state dict for this param\n",
        "\n",
        "                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n",
        "                    #if self.first_run_check==0:\n",
        "                        #self.first_run_check=1\n",
        "                        #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                    #look ahead weight storage now in state dict \n",
        "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
        "                    state['slow_buffer'].copy_(p.data)\n",
        "\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                #begin computations \n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                #compute variance mov avg\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                #compute mean moving avg\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "\n",
        "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > self.N_sma_threshhold:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "\n",
        "                if N_sma > self.N_sma_threshhold:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                else:\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "                #integrated look ahead...\n",
        "                #we do it at the param level instead of group level\n",
        "                if state['step'] % group['k'] == 0:\n",
        "                    slow_p = state['slow_buffer'] #get access to slow param tensor\n",
        "                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n",
        "                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "teWNlCsAV8r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Helper Functions"
      ],
      "metadata": {
        "id": "qWuBiD1IWJsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_depth(depth, cmap=cv2.COLORMAP_JET):\n",
        "    \"\"\"\n",
        "    depth: (H, W)\n",
        "    \"\"\"\n",
        "    x = depth.cpu().numpy()\n",
        "    x = np.nan_to_num(x) # change nan to 0\n",
        "    mi = np.min(x[x>0]) # get minimum positive depth (ignore background)\n",
        "    ma = np.max(x)\n",
        "    x = (x-mi)/(ma-mi+1e-8) # normalize to 0~1\n",
        "    x = (255*x).astype(np.uint8)\n",
        "    x_ = Image.fromarray(cv2.applyColorMap(x, cmap))\n",
        "    x_ = T.ToTensor()(x_) # (3, H, W)\n",
        "    return x_\n",
        "\n",
        "def visualize_prob(prob, cmap=cv2.COLORMAP_BONE):\n",
        "    \"\"\"\n",
        "    prob: (H, W) 0~1\n",
        "    \"\"\"\n",
        "    x = (255*prob).cpu().numpy().astype(np.uint8)\n",
        "    x_ = Image.fromarray(cv2.applyColorMap(x, cmap))\n",
        "    x_ = T.ToTensor()(x_) # (3, H, W)\n",
        "    return x_"
      ],
      "metadata": {
        "id": "tBkRAOACWMik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warmup Scheduler Function"
      ],
      "metadata": {
        "id": "Uk3tPXoxWWE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
        "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        multiplier: target learning rate = base lr * multiplier\n",
        "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
        "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        if self.multiplier < 1.:\n",
        "            raise ValueError('multiplier should be greater thant or equal to 1.')\n",
        "        self.total_epoch = total_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "\n",
        "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
        "        if self.last_epoch <= self.total_epoch:\n",
        "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(metrics, None)\n",
        "            else:\n",
        "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
        "            if self.finished and self.after_scheduler:\n",
        "                if epoch is None:\n",
        "                    self.after_scheduler.step(None)\n",
        "                else:\n",
        "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
        "            else:\n",
        "                return super(GradualWarmupScheduler, self).step(epoch)\n",
        "        else:\n",
        "            self.step_ReduceLROnPlateau(metrics, epoch)"
      ],
      "metadata": {
        "id": "TgNgneF_WcnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Helper Functions"
      ],
      "metadata": {
        "id": "_oI2l3sBWrrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(hparams, model):\n",
        "    eps = 1e-7 if hparams.use_amp else 1e-8\n",
        "    if hparams.optimizer == 'sgd':\n",
        "        optimizer = SGD(model.parameters(), lr=hparams.lr, \n",
        "                        momentum=hparams.momentum, weight_decay=hparams.weight_decay)\n",
        "    elif hparams.optimizer == 'adam':\n",
        "        optimizer = Adam(model.parameters(), lr=hparams.lr, eps=eps, \n",
        "                         weight_decay=hparams.weight_decay)\n",
        "    elif hparams.optimizer == 'radam':\n",
        "        optimizer = RAdam(model.parameters(), lr=hparams.lr, eps=eps, \n",
        "                          weight_decay=hparams.weight_decay)\n",
        "    elif hparams.optimizer == 'ranger':\n",
        "        optimizer = Ranger(model.parameters(), lr=hparams.lr, eps=eps, \n",
        "                          weight_decay=hparams.weight_decay)\n",
        "    else:\n",
        "        raise ValueError('optimizer not recognized!')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def get_scheduler(hparams, optimizer):\n",
        "    eps = 1e-7 if hparams.use_amp else 1e-8\n",
        "    if hparams.lr_scheduler == 'steplr':\n",
        "        scheduler = MultiStepLR(optimizer, milestones=hparams.decay_step, \n",
        "                                gamma=hparams.decay_gamma)\n",
        "    elif hparams.lr_scheduler == 'cosine':\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=hparams.num_epochs, eta_min=eps)\n",
        "    elif hparams.lr_scheduler == 'poly':\n",
        "        scheduler = LambdaLR(optimizer, \n",
        "                             lambda epoch: (1-epoch/hparams.num_epochs)**hparams.poly_exp)\n",
        "    else:\n",
        "        raise ValueError('scheduler not recognized!')\n",
        "\n",
        "    if hparams.warmup_epochs > 0 and hparams.optimizer not in ['radam', 'ranger']:\n",
        "        scheduler = GradualWarmupScheduler(optimizer, multiplier=hparams.warmup_multiplier, \n",
        "                                           total_epoch=hparams.warmup_epochs, after_scheduler=scheduler)\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def extract_model_state_dict(ckpt_path, prefixes_to_ignore=[]):\n",
        "    checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
        "    checkpoint_ = {}\n",
        "    if 'state_dict' in checkpoint: # if it's a pytorch-lightning checkpoint\n",
        "        for k, v in checkpoint['state_dict'].items():\n",
        "            if not k.startswith('model.'):\n",
        "                continue\n",
        "            k = k[6:] # remove 'model.'\n",
        "            for prefix in prefixes_to_ignore:\n",
        "                if k.startswith(prefix):\n",
        "                    print('ignore', k)\n",
        "                    break\n",
        "            else:\n",
        "                checkpoint_[k] = v\n",
        "    else: # if it only has model weights\n",
        "        for k, v in checkpoint.items():\n",
        "            for prefix in prefixes_to_ignore:\n",
        "                if k.startswith(prefix):\n",
        "                    print('ignore', k)\n",
        "                    break\n",
        "            else:\n",
        "                checkpoint_[k] = v\n",
        "    return checkpoint_\n",
        "\n",
        "def load_ckpt(model, ckpt_path, prefixes_to_ignore=[]):\n",
        "    model_dict = model.state_dict()\n",
        "    checkpoint_ = extract_model_state_dict(ckpt_path, prefixes_to_ignore)\n",
        "    model_dict.update(checkpoint_)\n",
        "    model.load_state_dict(model_dict)"
      ],
      "metadata": {
        "id": "DdRYtrT4Wu3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DTU Dataset Processing"
      ],
      "metadata": {
        "id": "YFgMFSzFVcO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "\n",
        "def read_pfm(filename):\n",
        "    file = open(filename, 'rb')\n",
        "    color = None\n",
        "    width = None\n",
        "    height = None\n",
        "    scale = None\n",
        "    endian = None\n",
        "    \n",
        "    header = file.readline().decode('utf-8').rstrip()\n",
        "    if header == 'PF':\n",
        "        color = True\n",
        "    elif header == 'Pf':\n",
        "        color = False\n",
        "    else:\n",
        "        raise Exception('Not a PFM file.')\n",
        "\n",
        "    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode('utf-8'))\n",
        "    if dim_match:\n",
        "        width, height = map(int, dim_match.groups())\n",
        "    else:\n",
        "        raise Exception('Malformed PFM header.')\n",
        "\n",
        "    scale = float(file.readline().rstrip())\n",
        "    if scale < 0:  # little-endian\n",
        "        endian = '<'\n",
        "        scale = -scale\n",
        "    else:\n",
        "        endian = '>'  # big-endian\n",
        "\n",
        "    data = np.fromfile(file, endian + 'f')\n",
        "    shape = (height, width, 3) if color else (height, width)\n",
        "\n",
        "    data = np.reshape(data, shape)\n",
        "    data = np.flipud(data)\n",
        "    file.close()\n",
        "    return data, scale\n",
        "\n",
        "\n",
        "def save_pfm(filename, image, scale=1):\n",
        "    file = open(filename, \"wb\")\n",
        "    color = None\n",
        "\n",
        "    image = np.flipud(image)\n",
        "\n",
        "    if image.dtype.name != 'float32':\n",
        "        raise Exception('Image dtype must be float32.')\n",
        "\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:  # color image\n",
        "        color = True\n",
        "    elif len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1:  # greyscale\n",
        "        color = False\n",
        "    else:\n",
        "        raise Exception('Image must have H x W x 3, H x W x 1 or H x W dimensions.')\n",
        "\n",
        "    file.write('PF\\n'.encode('utf-8') if color else 'Pf\\n'.encode('utf-8'))\n",
        "    file.write('{} {}\\n'.format(image.shape[1], image.shape[0]).encode('utf-8'))\n",
        "\n",
        "    endian = image.dtype.byteorder\n",
        "\n",
        "    if endian == '<' or endian == '=' and sys.byteorder == 'little':\n",
        "        scale = -scale\n",
        "\n",
        "    file.write(('%f\\n' % scale).encode('utf-8'))\n",
        "\n",
        "    image.tofile(file)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "reO1iPn_tMTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DTUDataset(Dataset):\n",
        "    def __init__(self, root_dir, split, n_views=3, n_depths=256, interval_scale=1.06):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        assert self.split in ['train', 'val'], \\\n",
        "            'split must be either \"train\" or \"val\"!'\n",
        "        self.build_metas()\n",
        "        self.n_views = n_views\n",
        "        self.n_depths = n_depths\n",
        "        self.interval_scale = interval_scale\n",
        "        self.build_proj_mats()\n",
        "        self.define_transforms()\n",
        "\n",
        "    def build_metas(self):\n",
        "        self.metas = []\n",
        "        with open(f'/content/dissertation_data/lists/dtu/{self.split}.txt') as f:\n",
        "            scans = [line.rstrip() for line in f.readlines()]\n",
        "\n",
        "        pair_file = \"Cameras/pair.txt\"\n",
        "        for scan in scans:\n",
        "            with open(os.path.join(self.root_dir, pair_file)) as f:\n",
        "                num_viewpoint = int(f.readline())\n",
        "                # viewpoints (49)\n",
        "                for _ in range(num_viewpoint):\n",
        "                    ref_view = int(f.readline().rstrip())\n",
        "                    src_views = [int(x) for x in f.readline().rstrip().split()[1::2]]\n",
        "                    # light conditions 0-6\n",
        "                    for light_idx in range(7):\n",
        "                        self.metas += [(scan, light_idx, ref_view, src_views)]\n",
        "\n",
        "    def build_proj_mats(self):\n",
        "        proj_mats = []\n",
        "        for vid in range(49): # total 49 view ids\n",
        "            proj_mat_filename = os.path.join(self.root_dir,\n",
        "                                             f'Cameras/train/{vid:08d}_cam.txt')\n",
        "            intrinsics, extrinsics, depth_min, depth_interval = \\\n",
        "                self.read_cam_file(proj_mat_filename)\n",
        "\n",
        "            # multiply intrinsics and extrinsics to get projection matrix\n",
        "            proj_mat = extrinsics\n",
        "            proj_mat[:3, :4] = intrinsics @ proj_mat[:3, :4]\n",
        "            proj_mats += [(torch.FloatTensor(proj_mat), depth_min, depth_interval)]\n",
        "\n",
        "        self.proj_mats = proj_mats\n",
        "\n",
        "    def read_cam_file(self, filename):\n",
        "        with open(filename) as f:\n",
        "            lines = [line.rstrip() for line in f.readlines()]\n",
        "        # extrinsics: line [1,5), 4x4 matrix\n",
        "        extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ')\n",
        "        extrinsics = extrinsics.reshape((4, 4))\n",
        "        # intrinsics: line [7-10), 3x3 matrix\n",
        "        intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ')\n",
        "        intrinsics = intrinsics.reshape((3, 3))\n",
        "        # depth_min & depth_interval: line 11\n",
        "        depth_min = float(lines[11].split()[0])\n",
        "        depth_interval = float(lines[11].split()[1]) * self.interval_scale\n",
        "        return intrinsics, extrinsics, depth_min, depth_interval\n",
        "\n",
        "    def read_depth(self, filename):\n",
        "        return np.array(read_pfm(filename)[0], dtype=np.float32)\n",
        "\n",
        "    def define_transforms(self):\n",
        "        if self.split == 'train':\n",
        "            self.transform = T.Compose([T.ToTensor(),\n",
        "                                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                                    std=[0.229, 0.224, 0.225]),\n",
        "                                       ])\n",
        "        else:\n",
        "            self.transform = T.Compose([T.ToTensor(),\n",
        "                                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                                                    std=[0.229, 0.224, 0.225]),\n",
        "                                       ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        scan, light_idx, ref_view, src_views = self.metas[idx]\n",
        "        # use only the reference view and first nviews-1 source views\n",
        "        view_ids = [ref_view] + src_views[:self.n_views-1]\n",
        "\n",
        "        imgs = []\n",
        "        proj_mats = []\n",
        "\n",
        "        for i, vid in enumerate(view_ids):\n",
        "            # NOTE that the id in image file names is from 1 to 49 (not 0~48)\n",
        "            img_filename = os.path.join(self.root_dir,\n",
        "                                f'Rectified/{scan}_train/rect_{vid+1:03d}_{light_idx}_r5000.png')\n",
        "            mask_filename = os.path.join(self.root_dir,\n",
        "                                f'Depths/{scan}_train/depth_visual_{vid:04d}.png')\n",
        "            depth_filename = os.path.join(self.root_dir,\n",
        "                                f'Depths/{scan}_train/depth_map_{vid:04d}.pfm')\n",
        "\n",
        "            img = Image.open(img_filename)\n",
        "            img = self.transform(img)\n",
        "            imgs += [img]\n",
        "\n",
        "            proj_mat, depth_min, depth_interval = self.proj_mats[vid]\n",
        "\n",
        "            if i == 0:  # reference view\n",
        "                depth_values = torch.arange(depth_min,\n",
        "                                            depth_interval*self.n_depths+depth_min,\n",
        "                                            depth_interval,\n",
        "                                            dtype=torch.float32)\n",
        "                mask = Image.open(mask_filename)\n",
        "                mask = torch.BoolTensor(np.array(mask))\n",
        "                depth = torch.FloatTensor(self.read_depth(depth_filename))\n",
        "                proj_mats += [torch.inverse(proj_mat)]\n",
        "            else:\n",
        "                proj_mats += [proj_mat]\n",
        "\n",
        "        imgs = torch.stack(imgs)\n",
        "        proj_mats = torch.stack(proj_mats)\n",
        "\n",
        "        return imgs, proj_mats, depth, depth_values, mask"
      ],
      "metadata": {
        "id": "4mHMpKioVNkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules"
      ],
      "metadata": {
        "id": "FecCtQOfkfJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBnReLU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, pad=1):\n",
        "        super(ConvBnReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.conv(x))\n",
        "\n",
        "class ConvBnReLU3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, pad=1):\n",
        "        super(ConvBnReLU3D, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.conv(x))\n",
        "\n",
        "def homo_warp(src_feat, src_proj, ref_proj_inv, depth_values):\n",
        "    # src_feat: (B, C, H, W)\n",
        "    # src_proj: (B, 4, 4)\n",
        "    # ref_proj_inv: (B, 4, 4)\n",
        "    # depth_values: (B, D)\n",
        "    # out: (B, C, D, H, W)\n",
        "    B, C, H, W = src_feat.shape\n",
        "    D = depth_values.shape[1]\n",
        "    device = src_feat.device\n",
        "    dtype = src_feat.dtype\n",
        "\n",
        "    transform = src_proj @ ref_proj_inv\n",
        "    R = transform[:, :3, :3] # (B, 3, 3)\n",
        "    T = transform[:, :3, 3:] # (B, 3, 1)\n",
        "    # create grid from the ref frame\n",
        "    ref_grid = create_meshgrid(H, W, normalized_coordinates=False) # (1, H, W, 2)\n",
        "    ref_grid = ref_grid.to(device).to(dtype)\n",
        "    ref_grid = ref_grid.permute(0, 3, 1, 2) # (1, 2, H, W)\n",
        "    ref_grid = ref_grid.reshape(1, 2, H*W) # (1, 2, H*W)\n",
        "    ref_grid = ref_grid.expand(B, -1, -1) # (B, 2, H*W)\n",
        "    ref_grid = torch.cat((ref_grid, torch.ones_like(ref_grid[:,:1])), 1) # (B, 3, H*W)\n",
        "    ref_grid_d = ref_grid.unsqueeze(2) * depth_values.view(B, 1, D, 1) # (B, 3, D, H*W)\n",
        "    ref_grid_d = ref_grid_d.view(B, 3, D*H*W)\n",
        "    src_grid_d = R @ ref_grid_d + T # (B, 3, D*H*W)\n",
        "    del ref_grid_d, ref_grid, transform, R, T # release (GPU) memory\n",
        "    src_grid = src_grid_d[:, :2] / src_grid_d[:, -1:] # divide by depth (B, 2, D*H*W)\n",
        "    del src_grid_d\n",
        "    src_grid[:, 0] = src_grid[:, 0]/((W - 1) / 2) - 1 # scale to -1~1\n",
        "    src_grid[:, 1] = src_grid[:, 1]/((H - 1) / 2) - 1 # scale to -1~1\n",
        "    src_grid = src_grid.permute(0, 2, 1) # (B, D*H*W, 2)\n",
        "    src_grid = src_grid.view(B, D, H*W, 2)\n",
        "\n",
        "    warped_src_feat = F.grid_sample(src_feat, src_grid,\n",
        "                                    mode='bilinear', padding_mode='zeros',\n",
        "                                    align_corners=True) # (B, C, D, H*W)\n",
        "    warped_src_feat = warped_src_feat.view(B, C, D, H, W)\n",
        "\n",
        "    return warped_src_feat\n",
        "\n",
        "def depth_regression(p, depth_values):\n",
        "    # p: probability volume [B, D, H, W]\n",
        "    # depth_values: discrete depth values [B, D]\n",
        "    depth_values = depth_values.view(*depth_values.shape, 1, 1)\n",
        "    depth = torch.sum(p * depth_values, 1)\n",
        "    return depth"
      ],
      "metadata": {
        "id": "o4PU8Zz_kgQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified MVSNet"
      ],
      "metadata": {
        "id": "omcmceERkoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureNet, self).__init__()\n",
        "        self.inplanes = 32\n",
        "\n",
        "        self.conv0 = ConvBnReLU(3, 8, 3, 1, 1)\n",
        "        self.conv1 = ConvBnReLU(8, 8, 3, 1, 1)\n",
        "\n",
        "        self.conv2 = ConvBnReLU(8, 16, 5, 2, 2)\n",
        "        self.conv3 = ConvBnReLU(16, 16, 3, 1, 1)\n",
        "        self.conv4 = ConvBnReLU(16, 16, 3, 1, 1)\n",
        "\n",
        "        self.conv5 = ConvBnReLU(16, 32, 5, 2, 2)\n",
        "        self.conv6 = ConvBnReLU(32, 32, 3, 1, 1)\n",
        "        self.feature = nn.Conv2d(32, 32, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(self.conv0(x))\n",
        "        x = self.conv4(self.conv3(self.conv2(x)))\n",
        "        x = self.feature(self.conv6(self.conv5(x)))\n",
        "        return x\n",
        "\n",
        "class CostRegNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CostRegNet, self).__init__()\n",
        "        self.conv0 = ConvBnReLU3D(32, 8)\n",
        "\n",
        "        self.conv1 = ConvBnReLU3D(8, 16, stride=2)\n",
        "        self.conv2 = ConvBnReLU3D(16, 16)\n",
        "\n",
        "        self.conv3 = ConvBnReLU3D(16, 32, stride=2)\n",
        "        self.conv4 = ConvBnReLU3D(32, 32)\n",
        "\n",
        "        self.conv5 = ConvBnReLU3D(32, 64, stride=2)\n",
        "        self.conv6 = ConvBnReLU3D(64, 64)\n",
        "\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2, bias=False),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(32, 16, kernel_size=3, padding=1, output_padding=1, stride=2, bias=False),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.conv11 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(16, 8, kernel_size=3, padding=1, output_padding=1, stride=2, bias=False),\n",
        "            nn.BatchNorm3d(8),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.prob = nn.Conv3d(8, 1, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv0 = self.conv0(x)\n",
        "        conv2 = self.conv2(self.conv1(conv0))\n",
        "        conv4 = self.conv4(self.conv3(conv2))\n",
        "        x = self.conv6(self.conv5(conv4))\n",
        "        x = conv4 + self.conv7(x)\n",
        "        x = conv2 + self.conv9(x)\n",
        "        x = conv0 + self.conv11(x)\n",
        "        x = self.prob(x)\n",
        "        return x\n",
        "\n",
        "class MVSNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MVSNet, self).__init__()\n",
        "        self.feature = FeatureNet()\n",
        "        self.cost_regularization = CostRegNet()\n",
        "\n",
        "    def forward(self, imgs, proj_mats, depth_values):\n",
        "        # imgs: (B, V, 3, H, W)\n",
        "        # proj_mats: (B, V, 4, 4)\n",
        "        # depth_values: (B, D)\n",
        "        B, V, _, H, W = imgs.shape\n",
        "        D = depth_values.shape[1]\n",
        "\n",
        "        # step 1. feature extraction\n",
        "        # in: images; out: 32-channel feature maps\n",
        "        imgs = imgs.reshape(B*V, 3, H, W)\n",
        "        feats = self.feature(imgs) # (B*V, F, h, w)\n",
        "        del imgs\n",
        "        feats = feats.reshape(B, V, *feats.shape[1:]) # (B, V, F, h, w)\n",
        "        ref_feats, src_feats = feats[:, 0], feats[:, 1:]\n",
        "        ref_proj, src_projs = proj_mats[:, 0], proj_mats[:, 1:]\n",
        "        src_feats = src_feats.permute(1, 0, 2, 3, 4) # (V-1, B, F, h, w)\n",
        "        src_projs = src_projs.permute(1, 0, 2, 3) # (V-1, B, 4, 4)\n",
        "\n",
        "        # step 2. differentiable homograph, build cost volume\n",
        "        ref_volume = ref_feats.unsqueeze(2).repeat(1, 1, D, 1, 1) # (B, F, D, h, w)\n",
        "        volume_sum = ref_volume\n",
        "        volume_sq_sum = ref_volume ** 2\n",
        "        del ref_volume\n",
        "\n",
        "        for src_feat, src_proj in zip(src_feats, src_projs):\n",
        "            warped_volume = homo_warp(src_feat, src_proj, ref_proj, depth_values)\n",
        "            volume_sum = volume_sum + warped_volume\n",
        "            volume_sq_sum = volume_sq_sum + warped_volume ** 2\n",
        "            del warped_volume\n",
        "        # aggregate multiple feature volumes by variance\n",
        "        volume_variance = volume_sq_sum.div_(V).sub_(volume_sum.div_(V).pow_(2))\n",
        "        del volume_sq_sum, volume_sum\n",
        "        \n",
        "        # step 3. cost volume regularization\n",
        "        cost_reg = self.cost_regularization(volume_variance).squeeze(1)\n",
        "        prob_volume = F.softmax(cost_reg, 1) # (B, D, h, w)\n",
        "        depth = depth_regression(prob_volume, depth_values)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # sum probability of 4 consecutive depth indices\n",
        "            prob_volume_sum4 = 4 * F.avg_pool3d(F.pad(prob_volume.unsqueeze(1),\n",
        "                                                      pad=(0, 0, 0, 0, 1, 2)),\n",
        "                                                (4, 1, 1), stride=1).squeeze(1) # (B, D, h, w)\n",
        "            # find the (rounded) index that is the final prediction\n",
        "            depth_index = depth_regression(prob_volume,\n",
        "                                           torch.arange(D,\n",
        "                                                        device=prob_volume.device,\n",
        "                                                        dtype=prob_volume.dtype)\n",
        "                                          ).long() # (B, h, w)\n",
        "            # the confidence is the 4-sum probability at this index\n",
        "            confidence = torch.gather(prob_volume_sum4, 1, \n",
        "                                      depth_index.unsqueeze(1)).squeeze(1) # (B, h, w)\n",
        "\n",
        "        return depth, confidence"
      ],
      "metadata": {
        "id": "sNkYT6TtksR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {\n",
        "    'root_dir': '/content/dissertation_data/MVS_TRAINING/dtu',\n",
        "    'n_views': 3,\n",
        "    'levels': 3,\n",
        "    'depth_interval': 2.65,\n",
        "    'n_depths': 256,\n",
        "    'interval_scale': 0.8,\n",
        "    # 'num_groups': [1, 2, 4, 8],\n",
        "    'num_groups': 1,\n",
        "    'loss_type': 'sl1',\n",
        "    'batch_size': 1,\n",
        "    'num_epochs': 5,\n",
        "    'num_gpus': 1,\n",
        "    'ckpt_path': '',\n",
        "    'prefixes_to_ignore': ['loss'],\n",
        "    'optimizer': 'sgd',\n",
        "    'lr': 1e-3,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 1e-5,\n",
        "    'lr_scheduler': 'steplr',\n",
        "    'warmup_multiplier': 1.0,\n",
        "    'warmup_epochs': 0,\n",
        "    'decay_step': [20],\n",
        "    'decay_gamma': 0.1,\n",
        "    'poly_exp': 0.9,\n",
        "    'use_amp': False,\n",
        "    'exp_name': 'exp'\n",
        "}\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "hparams = dotdict(hparams)\n",
        "\n",
        "hparams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPoXhyZHbPB2",
        "outputId": "e1e52c54-2ae3-4ff7-f6ab-7710287e050f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1,\n",
              " 'ckpt_path': '',\n",
              " 'decay_gamma': 0.1,\n",
              " 'decay_step': [20],\n",
              " 'depth_interval': 2.65,\n",
              " 'exp_name': 'exp',\n",
              " 'interval_scale': 0.8,\n",
              " 'levels': 3,\n",
              " 'loss_type': 'sl1',\n",
              " 'lr': 0.001,\n",
              " 'lr_scheduler': 'steplr',\n",
              " 'momentum': 0.9,\n",
              " 'n_depths': 256,\n",
              " 'n_views': 3,\n",
              " 'num_epochs': 5,\n",
              " 'num_gpus': 1,\n",
              " 'num_groups': 1,\n",
              " 'optimizer': 'sgd',\n",
              " 'poly_exp': 0.9,\n",
              " 'prefixes_to_ignore': ['loss'],\n",
              " 'root_dir': '/content/dissertation_data/MVS_TRAINING/dtu',\n",
              " 'use_amp': False,\n",
              " 'warmup_epochs': 0,\n",
              " 'warmup_multiplier': 1.0,\n",
              " 'weight_decay': 1e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "torch.backends.cudnn.benchmark = True # this increases training speed by 5x\n",
        "\n",
        "class MVSSystem(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(MVSSystem, self).__init__()\n",
        "        # self.hparams = hparams\n",
        "        # to unnormalize image for visualization\n",
        "        self.unpreprocess = T.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], \n",
        "                                        std=[1/0.229, 1/0.224, 1/0.225])\n",
        "\n",
        "        self.loss = loss_dict[hparams.loss_type](ohem=True, topk=0.6)\n",
        "\n",
        "        self.model = MVSNet()\n",
        "\n",
        "        # if num gpu is 1, print model structure and number of params\n",
        "        if hparams.num_gpus == 1:\n",
        "            # print(self.model)\n",
        "            print('number of parameters : %.2f M' % \n",
        "                  (sum(p.numel() for p in self.model.parameters() if p.requires_grad) / 1e6))\n",
        "        \n",
        "        # load model if checkpoint path is provided\n",
        "        if hparams.ckpt_path != '':\n",
        "            print('Load model from', hparams.ckpt_path)\n",
        "            load_ckpt(self.model, hparams.ckpt_path, hparams.prefixes_to_ignore)\n",
        "\n",
        "    def forward(self, imgs, proj_mats, depth_values):\n",
        "        return self.model(imgs, proj_mats, depth_values)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        imgs, proj_mats, depth_gt, depth_values, mask = batch\n",
        "        depth_pred, confidence = self.forward(imgs, proj_mats, depth_values)\n",
        "        loss = self.loss(depth_pred, depth_gt, mask)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if batch_nb == 0:\n",
        "                img_ = self.unpreprocess(imgs[0,0,:,::4,::4]).cpu() # batch 0, ref image, 1/4 scale\n",
        "                depth_gt_ = visualize_depth(depth_gt[0])\n",
        "                depth_pred_ = visualize_depth(depth_pred[0]*mask[0])\n",
        "                prob = visualize_prob(confidence[0]*mask[0])\n",
        "                stack = torch.stack([img_, depth_gt_, depth_pred_, prob]) # (4, 3, H, W)\n",
        "                self.logger.experiment.add_images('train/image_GT_pred_prob',\n",
        "                                                  stack, self.global_step)\n",
        "\n",
        "            abs_err = abs_error(depth_pred, depth_gt, mask).mean()\n",
        "            acc_1mm = acc_threshold(depth_pred, depth_gt, mask, 1).mean()\n",
        "            acc_2mm = acc_threshold(depth_pred, depth_gt, mask, 2).mean()\n",
        "            acc_4mm = acc_threshold(depth_pred, depth_gt, mask, 4).mean()\n",
        "\n",
        "        return {'loss': loss,\n",
        "                'progress_bar': {'train_abs_err': abs_err},\n",
        "                'log': {'train/loss': loss,\n",
        "                        'train/abs_err': abs_err,\n",
        "                        'train/acc_1mm': acc_1mm,\n",
        "                        'train/acc_2mm': acc_2mm,\n",
        "                        'train/acc_4mm': acc_4mm,\n",
        "                        'lr': get_learning_rate(self.optimizer)}\n",
        "               }\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        imgs, proj_mats, depth_gt, depth_values, mask = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            depth_pred, confidence = self.forward(imgs, proj_mats, depth_values)\n",
        "            loss = self.loss(depth_pred, depth_gt, mask)\n",
        "\n",
        "            if batch_nb == 0:\n",
        "                img_ = self.unpreprocess(imgs[0,0,:,::4,::4]).cpu() # batch 0, ref image, 1/4 scale\n",
        "                depth_gt_ = visualize_depth(depth_gt[0])\n",
        "                depth_pred_ = visualize_depth(depth_pred[0]*mask[0])\n",
        "                prob = visualize_prob(confidence[0]*mask[0])\n",
        "                stack = torch.stack([img_, depth_gt_, depth_pred_, prob]) # (4, 3, H, W)\n",
        "                self.logger.experiment.add_images('val/image_GT_pred_prob',\n",
        "                                                  stack, self.global_step)\n",
        "\n",
        "            abs_err = abs_error(depth_pred, depth_gt, mask)\n",
        "            acc_1mm = acc_threshold(depth_pred, depth_gt, mask, 1)\n",
        "            acc_2mm = acc_threshold(depth_pred, depth_gt, mask, 2)\n",
        "            acc_4mm = acc_threshold(depth_pred, depth_gt, mask, 4)\n",
        "\n",
        "        return {'val_loss': loss,\n",
        "                'val_abs_err': abs_err,\n",
        "                'val_acc_1mm': acc_1mm,\n",
        "                'val_acc_2mm': acc_2mm,\n",
        "                'val_acc_4mm': acc_4mm,\n",
        "                }\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        mean_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        mean_abs_err = torch.cat([x['val_abs_err'] for x in outputs]).mean()\n",
        "        mean_acc_1mm = torch.cat([x['val_acc_1mm'] for x in outputs]).mean()\n",
        "        mean_acc_2mm = torch.cat([x['val_acc_2mm'] for x in outputs]).mean()\n",
        "        mean_acc_4mm = torch.cat([x['val_acc_4mm'] for x in outputs]).mean()\n",
        "\n",
        "        return {'progress_bar': {'val_loss': mean_loss,\n",
        "                                 'val_abs_err': mean_abs_err},\n",
        "                'log': {'val/loss': mean_loss,\n",
        "                        'val/abs_err': mean_abs_err,\n",
        "                        'val/acc_1mm': mean_acc_1mm,\n",
        "                        'val/acc_2mm': mean_acc_2mm,\n",
        "                        'val/acc_4mm': mean_acc_4mm,\n",
        "                        }\n",
        "               }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = get_optimizer(hparams, self.model)\n",
        "        scheduler = get_scheduler(hparams, self.optimizer)\n",
        "        \n",
        "        return [self.optimizer], [scheduler]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = DTUDataset(root_dir=hparams.root_dir,\n",
        "                                   split='train',\n",
        "                                   n_views=hparams.n_views,\n",
        "                                   n_depths=hparams.n_depths,\n",
        "                                   interval_scale=hparams.interval_scale)\n",
        "        # if self.hparams.num_gpus > 1:\n",
        "        #     sampler = DistributedSampler(train_dataset)\n",
        "        # else:\n",
        "        #     sampler = None\n",
        "        return DataLoader(train_dataset, \n",
        "                          shuffle=True,\n",
        "                          num_workers=4,\n",
        "                          batch_size=hparams.batch_size,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataset = DTUDataset(root_dir=hparams.root_dir,\n",
        "                                 split='val',\n",
        "                                 n_views=hparams.n_views,\n",
        "                                 n_depths=hparams.n_depths,\n",
        "                                 interval_scale=hparams.interval_scale)\n",
        "        # if self.hparams.num_gpus > 1:\n",
        "        #     sampler = DistributedSampler(val_dataset)\n",
        "        # else:\n",
        "        #     sampler = None\n",
        "        return DataLoader(val_dataset, \n",
        "                          shuffle=False,\n",
        "                          num_workers=4,\n",
        "                          batch_size=hparams.batch_size,\n",
        "                          pin_memory=True)\n",
        "        \n",
        "\n",
        "system = MVSSystem(hparams)\n",
        "# checkpoint_callback = ModelCheckpoint(dirpath='logs/exp',\n",
        "#                                         monitor='val/loss',\n",
        "#                                         mode='min',\n",
        "#                                         save_top_k=1,)\n",
        "\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=\"logs\",\n",
        "    name=hparams.exp_name,\n",
        "    # debug=False,\n",
        "    # create_git_tag=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(max_epochs=hparams.num_epochs,\n",
        "                    # checkpoint_callback=checkpoint_callback,\n",
        "                    logger=logger,\n",
        "                    # tpu_cores=1,\n",
        "                    # early_stop_callback=None,\n",
        "                    # weights_summary=None,\n",
        "                    gpus=hparams.num_gpus,\n",
        "                    # distributed_backend='ddp',\n",
        "                    num_sanity_val_steps=0,\n",
        "                    # progress_bar_refresh_rate=1  \n",
        "                    # use_amp=hparams.use_amp,\n",
        "                    # amp_level='O1'\n",
        "                    )\n",
        "\n",
        "trainer.fit(system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "f353f99404084e4a9f7a87d79bb38684",
            "7041ad2144064f13bc5bb31f35b7f2fd",
            "6ac3c60b187248348cfb63c095216cb8",
            "1fff411e47a14fe988c78c1322529c34",
            "e374a921a4dc4933b39a3694879a9f66",
            "36f1f2a361b14fe488d2c0ca63be6e75",
            "075537718d9f4467bf4cd12fc9f58362",
            "faca4d3a928346d1b6e775b7caf7e0ce",
            "df0f266606d547f799afd4869faff584",
            "940e04a719c94f798503bae8994b50ce",
            "575b0b08dca14a369237dbdc6eed19d8"
          ]
        },
        "id": "2tLo9xuldVtf",
        "outputId": "93699077-9dd5-44c6-8b43-786730fd7d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters : 0.34 M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name         | Type      | Params\n",
            "-------------------------------------------\n",
            "0 | unpreprocess | Normalize | 0     \n",
            "1 | loss         | SL1Loss   | 0     \n",
            "2 | model        | MVSNet    | 338 K \n",
            "-------------------------------------------\n",
            "338 K     Trainable params\n",
            "0         Non-trainable params\n",
            "338 K     Total params\n",
            "1.353     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f353f99404084e4a9f7a87d79bb38684"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sQIOhkKTlqCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}